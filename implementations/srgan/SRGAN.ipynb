{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRGAN ( Super Resolution Generative Adversarial Network )\n",
    "\n",
    "---\n",
    "\n",
    "# 목차\n",
    "### - 소개\n",
    "### - 이론정리\n",
    "### - 주요 소스 분석\n",
    "### - 결과 ( 비교분석 )\n",
    "### - 결론\n",
    "### - 참고문헌\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소개\n",
    "\n",
    "### - 저해상도 (LR)에서 고해상도 (HR) 이미지를 추정하는 매우 어려운 작업을 SR(Super-resolution)이라고 한다.\n",
    "### - 많은 분야에서 저해상도 이미지를 고해상도로 만드는 작업을 하고 있다.\n",
    "### - GAN을 사용하여 SR을 할 경우 효과가 굉장히 높다는 것을 발견함\n",
    "### - SRGAN ( Super-resolution GAN )이 등장함\n",
    "### - 그렇기에 GAN을 통하여 SR을 한 이미지가 기존 다른 방법들과 얼만큼의 차이가 있는지를 파악하고자 함\n",
    "\n",
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이론정리\n",
    "## - MSE & PSNR\n",
    " - <h3>기존 SR 알고리즘의 최적화 목표는 일반적으로 복구 된 SR 이미지와 정답 ( Original HR Image) 사이의 평균 제곱 오차 (MSE)를 최소화 하는 것이다. </h3>\n",
    " - <h3>이유는 MSE를 최소화 하면 SR 알고리즘을 평가하고 비교하는데 사용되는 일반적인 측정치인 PSNR ( peak signal-to-noise ratio )를 최대화하기 때문이다.</h3>\n",
    " \n",
    "<p align=\"center\">\n",
    "    <img src=\"./images_ipykernel/PSNR_MSE.jpg\"\\>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Interpolation\n",
    " - <h3>Interpolation(보간)이란 알려진 지점의 값 사이에 위치한 값을 알려진 값으로부터 추정하는 것</h3>\n",
    " - <h3>Interpolation과 대비되는 용어로 extrapolation이 있는데, 이는 알려진 값들 사이의 값이 아닌 범위를 벗어난 외부의 위치에서의 값을 추정하는 것을 말한다.</h3>\n",
    "\n",
    " \n",
    "<p align=\"center\">\n",
    "    <img src=\"./images_ipykernel/interpolation.jpg\"\\>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator & Discriminator 모델 구조\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images_ipykernel/Discriminator_Ganerator.jpg\"\\>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"./images_ipykernel/paper_image.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "### - PSNR이 높을수록 원본 이미지와의 차이가 적다는 것인데 위의 그림을 보면 PSNR이 가장 낮은 SRGAN이 육안으로 봤을 때 가장 원본 이미지와 일치함을 느낄 수 있다.\n",
    "### - 그렇기 때문에 PSNR과 MSE가 디테일에 관련된 부분을 생각하면 절대적인 측정방법이 아니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ganerator\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/Loss.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "---\n",
    "# Discriminator\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/Discriminator%20loss.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "---\n",
    "## Perceptual loss function\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/Perceptual%20loss%20function.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 기존 MSE를 사용한 content loss\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/MSE%20Loss%20function.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## MSE를 하기전에 VGG19 Model을 사용하여 Feature Extract를 통하여 나온 Feature map을 가지고 MSE하는 content loss\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/paper%20content%20loss.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/VGG19.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Adversarial loss\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/Adversarial%20loss.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## SRGAN에서 Content loss에서 사용될 Feature map위치에 따른 차이\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/FeatureMap_result.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 논문에서의 결과화면 ( 비교 )\n",
    "<p align=\"center\">\n",
    "    <img src=\"images_ipykernel/result1.jpg\"\\>\n",
    "    <img src=\"images_ipykernel/result2.jpg\"\\>\n",
    "    <img src=\"images_ipykernel/result3.jpg\"\\>\n",
    "    <img src=\"images_ipykernel/result4.jpg\"\\>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch관련 설치법\n",
    "## - 환경설정 ( 설치 )\n",
    " - <h3>cuda toolkit 10.1</h3>\n",
    " - <h3>cudnn 7.6.5 for cuda toolkit 10.1</h3>\n",
    " - <h3>그래픽 드라이버 (NVIDIA)</h3>\n",
    " - <h3>Anaconda (가상환경을 사용해서 할 경우) 권장</h3>\n",
    "\n",
    "## 설치 필요 패키지\n",
    " - <h3>python version = 3.7</h3>\n",
    " - <h3>conda install pytorch torchvision cudatoolkit-10.1 -c pytorch</h3>\n",
    " - <h3>pip install torchsummary (모델 구조를 쉽게 보기 위함</h3>\n",
    " - <h3>pip install opencv-python</h3>\n",
    " - <h3>conda install matplotlib</h3>\n",
    " - <h3>etc ... ( 그 외 필요 라이브러리를 설치하여 사용하면 됩니다 ) </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소스 분석\n",
    "## Dataset's DataLoader customizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, hr_shape, max_index): # download, read data 등등을 하는 파트\n",
    "        hr_height, hr_width = hr_shape # 256 X 256\n",
    "        self.max_index = max_index\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        self.lr_transform = transforms.Compose( # 고해상도로 만들 데이터인 low resolution data\n",
    "            [\n",
    "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC), # 기본 width, height에서 4를 나눈 값으로 resize\n",
    "                transforms.ToTensor(), # image값을 tensor형태로 변환\n",
    "                transforms.Normalize(mean, std), # 위에서 선언한 mean, std값을 사용하여 0~1사이로 Normalize\n",
    "            ]\n",
    "        )\n",
    "        self.hr_transform = transforms.Compose( # high resolution image 데이터\n",
    "            [\n",
    "                transforms.Resize((hr_height, hr_height), Image.BICUBIC), # BICUBIC : 주변 16개의 픽셀을 사용하여 처리\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(root + \"/*.*\")) # 오름차순으로 파일 정렬\n",
    "\n",
    "    def __getitem__(self, index): # 인덱스에 해당하는 아이템을 넘겨주는 파트\n",
    "        if index <= self.max_index:\n",
    "            img = Image.open(self.files[index % len(self.files)])\n",
    "            img_lr = self.lr_transform(img)\n",
    "            img_hr = self.hr_transform(img)\n",
    "        \n",
    "            return {\"lr\": img_lr, \"hr\": img_hr} # map 형태로 반환\n",
    "\n",
    "    def __len__(self): # data size를 넘겨주는 파트\n",
    "        return self.max_index # 파일 길이 반환 ( 총 이미지 수 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.models import vgg19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module): # Feature Extract Model\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True) # pretrained된 vgg19 model\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:36])\n",
    "\n",
    "    def forward(self, img): # forward\n",
    "        return self.feature_extractor(img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 Model 구성도\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images_ipykernel/VGG19.jpg\"\\>\n",
    "</p>\n",
    "\n",
    "## 몇번째 Conv에서 나온 Feature map을 사용하는 것이 의미가 있는가 ?\n",
    "### - 논문에서는 보다 깊은 네트워크의 레이어에서 나온 Feature map이 이미지의 content에 보다 잠재적으로 집중한다고 언급함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19 Model Summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-5         [-1, 64, 128, 128]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "            Conv2d-8        [-1, 128, 128, 128]         147,584\n",
      "              ReLU-9        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-10          [-1, 128, 64, 64]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]         295,168\n",
      "             ReLU-12          [-1, 256, 64, 64]               0\n",
      "           Conv2d-13          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-16          [-1, 256, 64, 64]               0\n",
      "           Conv2d-17          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-18          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-19          [-1, 256, 32, 32]               0\n",
      "           Conv2d-20          [-1, 512, 32, 32]       1,180,160\n",
      "             ReLU-21          [-1, 512, 32, 32]               0\n",
      "           Conv2d-22          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-23          [-1, 512, 32, 32]               0\n",
      "           Conv2d-24          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-25          [-1, 512, 32, 32]               0\n",
      "           Conv2d-26          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 512, 16, 16]               0\n",
      "           Conv2d-29          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-30          [-1, 512, 16, 16]               0\n",
      "           Conv2d-31          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-32          [-1, 512, 16, 16]               0\n",
      "           Conv2d-33          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-34          [-1, 512, 16, 16]               0\n",
      "           Conv2d-35          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-36          [-1, 512, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 311.00\n",
      "Params size (MB): 76.39\n",
      "Estimated Total Size (MB): 388.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"VGG19 Model Summary\")\n",
    "summary(FeatureExtractor().cuda(),input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문에서 VGG19의 5번째 maxpooling전의 4번째 convolution(활성함수 이후까지)를 가지고 Feature extract를 하는 것이 가장 성능이 좋다고 언급함.\n",
    "\n",
    "### 따라서 해당 부분까지의 pre-train된 model을 가져와서 사용함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module): # ResNet BasicBlock\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1), # 3X3 conv filter = same\n",
    "            nn.BatchNorm2d(in_features, 0.8), # batch normalization\n",
    "            nn.PReLU(), # PReLU => y = ax (if a = 0.1 ==> Leaky ReLU) 여기서 a(알파)는 학습을 통해 결정됨\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1), # 3X3 conv filter = same\n",
    "            nn.BatchNorm2d(in_features, 0.8), # batch normalization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = x + self.conv_block(x)\n",
    "        return result # concat ( skip connection )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualBlock 구성도\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images_ipykernel/resnet_block.jpg\"\\>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock Model Summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "             PReLU-3         [-1, 64, 256, 256]               1\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "================================================================\n",
      "Total params: 74,113\n",
      "Trainable params: 74,113\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 16.00\n",
      "Forward/backward pass size (MB): 160.00\n",
      "Params size (MB): 0.28\n",
      "Estimated Total Size (MB): 176.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"ResidualBlock Model Summary\")\n",
    "summary(ResidualBlock(64).cuda(),input_size=(64,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module): # 생성자 ( Generator )\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):  # Low resolution Tensor를 사용하여 High resolution을 생성\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n",
    "\n",
    "        # Residual blocks\n",
    "        res_blocks = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            res_blocks.append(ResidualBlock(64))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "\n",
    "        # Second conv layer post residual blocks\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n",
    "\n",
    "        # Upsampling layers\n",
    "        upsampling = []\n",
    "        for out_features in range(2):\n",
    "            upsampling += [\n",
    "                # nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2), # upscale\n",
    "                nn.PReLU(),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "\n",
    "        # Final output layer\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1) # total 16개의 Residual_block으로 구성됨\n",
    "        out2 = self.conv2(out)\n",
    "        #print(\"out1 result : \",out1.shape)\n",
    "        #print(\"out2 result : \",out2.shape)\n",
    "        out = torch.add(out1, out2) # 덧셈 concat가 아닌 value 덧셈\n",
    "        #print(\"torch.add result : \",out.shape)\n",
    "        out = self.upsampling(out) # 2번 반복 64 -> 128  // 128-> 256 순으로\n",
    "        out = self.conv3(out) # channel 64 -> 3 (super resolution 마지막 단계)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Model Summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]          15,616\n",
      "             PReLU-2           [-1, 64, 64, 64]               1\n",
      "            Conv2d-3           [-1, 64, 64, 64]          36,928\n",
      "       BatchNorm2d-4           [-1, 64, 64, 64]             128\n",
      "             PReLU-5           [-1, 64, 64, 64]               1\n",
      "            Conv2d-6           [-1, 64, 64, 64]          36,928\n",
      "       BatchNorm2d-7           [-1, 64, 64, 64]             128\n",
      "     ResidualBlock-8           [-1, 64, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-10           [-1, 64, 64, 64]             128\n",
      "            PReLU-11           [-1, 64, 64, 64]               1\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-16           [-1, 64, 64, 64]             128\n",
      "            PReLU-17           [-1, 64, 64, 64]               1\n",
      "           Conv2d-18           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-20           [-1, 64, 64, 64]               0\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "            PReLU-23           [-1, 64, 64, 64]               1\n",
      "           Conv2d-24           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-25           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-26           [-1, 64, 64, 64]               0\n",
      "           Conv2d-27           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-28           [-1, 64, 64, 64]             128\n",
      "            PReLU-29           [-1, 64, 64, 64]               1\n",
      "           Conv2d-30           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-31           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-32           [-1, 64, 64, 64]               0\n",
      "           Conv2d-33           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-34           [-1, 64, 64, 64]             128\n",
      "            PReLU-35           [-1, 64, 64, 64]               1\n",
      "           Conv2d-36           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-37           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-38           [-1, 64, 64, 64]               0\n",
      "           Conv2d-39           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-40           [-1, 64, 64, 64]             128\n",
      "            PReLU-41           [-1, 64, 64, 64]               1\n",
      "           Conv2d-42           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-43           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-44           [-1, 64, 64, 64]               0\n",
      "           Conv2d-45           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-46           [-1, 64, 64, 64]             128\n",
      "            PReLU-47           [-1, 64, 64, 64]               1\n",
      "           Conv2d-48           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-49           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-50           [-1, 64, 64, 64]               0\n",
      "           Conv2d-51           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-52           [-1, 64, 64, 64]             128\n",
      "            PReLU-53           [-1, 64, 64, 64]               1\n",
      "           Conv2d-54           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-55           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-56           [-1, 64, 64, 64]               0\n",
      "           Conv2d-57           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-58           [-1, 64, 64, 64]             128\n",
      "            PReLU-59           [-1, 64, 64, 64]               1\n",
      "           Conv2d-60           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-61           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-62           [-1, 64, 64, 64]               0\n",
      "           Conv2d-63           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-64           [-1, 64, 64, 64]             128\n",
      "            PReLU-65           [-1, 64, 64, 64]               1\n",
      "           Conv2d-66           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-67           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-68           [-1, 64, 64, 64]               0\n",
      "           Conv2d-69           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-70           [-1, 64, 64, 64]             128\n",
      "            PReLU-71           [-1, 64, 64, 64]               1\n",
      "           Conv2d-72           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-73           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-74           [-1, 64, 64, 64]               0\n",
      "           Conv2d-75           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-76           [-1, 64, 64, 64]             128\n",
      "            PReLU-77           [-1, 64, 64, 64]               1\n",
      "           Conv2d-78           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-79           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-80           [-1, 64, 64, 64]               0\n",
      "           Conv2d-81           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-82           [-1, 64, 64, 64]             128\n",
      "            PReLU-83           [-1, 64, 64, 64]               1\n",
      "           Conv2d-84           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-85           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-86           [-1, 64, 64, 64]               0\n",
      "           Conv2d-87           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-88           [-1, 64, 64, 64]             128\n",
      "            PReLU-89           [-1, 64, 64, 64]               1\n",
      "           Conv2d-90           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-91           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-92           [-1, 64, 64, 64]               0\n",
      "           Conv2d-93           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-94           [-1, 64, 64, 64]             128\n",
      "            PReLU-95           [-1, 64, 64, 64]               1\n",
      "           Conv2d-96           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-97           [-1, 64, 64, 64]             128\n",
      "    ResidualBlock-98           [-1, 64, 64, 64]               0\n",
      "           Conv2d-99           [-1, 64, 64, 64]          36,928\n",
      "     BatchNorm2d-100           [-1, 64, 64, 64]             128\n",
      "          Conv2d-101          [-1, 256, 64, 64]         147,712\n",
      "     BatchNorm2d-102          [-1, 256, 64, 64]             512\n",
      "    PixelShuffle-103         [-1, 64, 128, 128]               0\n",
      "           PReLU-104         [-1, 64, 128, 128]               1\n",
      "          Conv2d-105        [-1, 256, 128, 128]         147,712\n",
      "     BatchNorm2d-106        [-1, 256, 128, 128]             512\n",
      "    PixelShuffle-107         [-1, 64, 256, 256]               0\n",
      "           PReLU-108         [-1, 64, 256, 256]               1\n",
      "          Conv2d-109          [-1, 3, 256, 256]          15,555\n",
      "            Tanh-110          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 1,550,486\n",
      "Trainable params: 1,550,486\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 363.00\n",
      "Params size (MB): 5.91\n",
      "Estimated Total Size (MB): 368.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Generator Model Summary\")\n",
    "summary(GeneratorResNet().cuda(),input_size=(3,256//4,256//4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 구성도\n",
    "---\n",
    "### 1 ~ 2 = conv1 block\n",
    "### 3~98 layer = Residual block ( total 16 Residual block )\n",
    "- Conv2d -> BatchNorm2d -> PReLU -> Conv2d -> BatchNorm2d = F(x)\n",
    "- Conv2d's input = x\n",
    "- F(x) + x = ResidualBlock ( 덧셈 Not concat )\n",
    "\n",
    "### 99 ~ 100 layer = conv2 block\n",
    "### 101~108 layer = upsampling \n",
    "### 109 & 110 layer = conv3 block\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module): # Dense를 하지 않고 바로 사용\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_withDense(nn.Module): # 바로 Dense하지 않고 Conv를 사용하여 차원압축을 한 후 Dense를 진행하는 방식\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator_withDense, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1)) # 논문 구조에는 없는 부분이지만 해당 부분 없이 바로\n",
    "                                                                                    #Dense를 할 경우 Param수가 급등함으로써 학습이 어려워짐.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        layers2 = []\n",
    "        layers2.append(nn.Linear(16*16,1024))\n",
    "        layers2.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers2.append(nn.Linear(1024,1))\n",
    "        layers2.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model2 = nn.Sequential(*layers2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.model(img)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        return self.model2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_withDense_paper(nn.Module): # 논문과 일치하도록 Dense를 사용\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator_withDense_paper, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        #layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1)) # 논문 구조에는 없는 부분이지만 해당 부분 없이 바로\n",
    "                                                                                    #Dense를 할 경우 Param수가 급등함으로써 학습이 어려워짐.\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        layers2 = []\n",
    "        layers2.append(nn.Linear(16*16*512,1024))\n",
    "        layers2.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers2.append(nn.Linear(1024,1))\n",
    "        layers2.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model2 = nn.Sequential(*layers2)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.model(img)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        return self.model2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Model Summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "         LeakyReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "         LeakyReLU-5         [-1, 64, 128, 128]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-7        [-1, 128, 128, 128]             256\n",
      "         LeakyReLU-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-10          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-11          [-1, 128, 64, 64]               0\n",
      "           Conv2d-12          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-13          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-16          [-1, 256, 32, 32]             512\n",
      "        LeakyReLU-17          [-1, 256, 32, 32]               0\n",
      "           Conv2d-18          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-19          [-1, 512, 32, 32]           1,024\n",
      "        LeakyReLU-20          [-1, 512, 32, 32]               0\n",
      "           Conv2d-21          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-22          [-1, 512, 16, 16]           1,024\n",
      "        LeakyReLU-23          [-1, 512, 16, 16]               0\n",
      "           Conv2d-24            [-1, 1, 16, 16]           4,609\n",
      "================================================================\n",
      "Total params: 4,693,697\n",
      "Trainable params: 4,693,697\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 193.00\n",
      "Params size (MB): 17.91\n",
      "Estimated Total Size (MB): 211.66\n",
      "----------------------------------------------------------------\n",
      "Discriminator_withDense Model Summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "         LeakyReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "         LeakyReLU-5         [-1, 64, 128, 128]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-7        [-1, 128, 128, 128]             256\n",
      "         LeakyReLU-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-10          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-11          [-1, 128, 64, 64]               0\n",
      "           Conv2d-12          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-13          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-16          [-1, 256, 32, 32]             512\n",
      "        LeakyReLU-17          [-1, 256, 32, 32]               0\n",
      "           Conv2d-18          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-19          [-1, 512, 32, 32]           1,024\n",
      "        LeakyReLU-20          [-1, 512, 32, 32]               0\n",
      "           Conv2d-21          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-22          [-1, 512, 16, 16]           1,024\n",
      "        LeakyReLU-23          [-1, 512, 16, 16]               0\n",
      "           Conv2d-24            [-1, 1, 16, 16]           4,609\n",
      "           Linear-25                 [-1, 1024]         263,168\n",
      "        LeakyReLU-26                 [-1, 1024]               0\n",
      "           Linear-27                    [-1, 1]           1,025\n",
      "          Sigmoid-28                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 4,957,890\n",
      "Trainable params: 4,957,890\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 193.02\n",
      "Params size (MB): 18.91\n",
      "Estimated Total Size (MB): 212.68\n",
      "----------------------------------------------------------------\n",
      "Discriminator_withDense_paper Model Summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "         LeakyReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "         LeakyReLU-5         [-1, 64, 128, 128]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-7        [-1, 128, 128, 128]             256\n",
      "         LeakyReLU-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-10          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-11          [-1, 128, 64, 64]               0\n",
      "           Conv2d-12          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-13          [-1, 256, 64, 64]             512\n",
      "        LeakyReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-16          [-1, 256, 32, 32]             512\n",
      "        LeakyReLU-17          [-1, 256, 32, 32]               0\n",
      "           Conv2d-18          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-19          [-1, 512, 32, 32]           1,024\n",
      "        LeakyReLU-20          [-1, 512, 32, 32]               0\n",
      "           Conv2d-21          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-22          [-1, 512, 16, 16]           1,024\n",
      "        LeakyReLU-23          [-1, 512, 16, 16]               0\n",
      "           Linear-24                 [-1, 1024]     134,218,752\n",
      "        LeakyReLU-25                 [-1, 1024]               0\n",
      "           Linear-26                    [-1, 1]           1,025\n",
      "          Sigmoid-27                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 138,908,865\n",
      "Trainable params: 138,908,865\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 193.02\n",
      "Params size (MB): 529.90\n",
      "Estimated Total Size (MB): 723.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Discriminator Model Summary\")\n",
    "summary(Discriminator(input_shape=(3, 256,256)).cuda(),input_size=(3,256,256))\n",
    "\n",
    "print(\"Discriminator_withDense Model Summary\")\n",
    "summary(Discriminator_withDense(input_shape=(3, 256,256)).cuda(),input_size=(3,256,256))\n",
    "\n",
    "print(\"Discriminator_withDense_paper Model Summary\")\n",
    "summary(Discriminator_withDense_paper(input_shape=(3, 256,256)).cuda(),input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator without Dense's total params          :     4,693,697\n",
    "## Discriminator with Dense and Conv total params :     4,957,890\n",
    "## Discriminator with Dense like a paper                    : 138,908,865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Model 구조\n",
    "### 첫번째 discriminator_block만 첫번째 Conv2d Layer이후 BatchNrom2d를 하지 않으며 나머지 block은 전부 수행한다. ( 차이점 )\n",
    "- 1 ~ 5 : discriminator_block first block\n",
    "- 6 ~ 11 : discriminator_block second block\n",
    "- 12 ~ 17 : discriminator_block third block\n",
    "- 18 ~ 23 : discriminator_block firth block\n",
    "- 24 : conv2d layer Not using activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import easydict #jupyter notebook 환경에서는 argparse를 사용할 수 없기 때문에 easydict으로 대체하여 설명\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'n_epochs': 200, 'dataset_name': 'img_align_celeba', 'batch_size': 8, 'lr': 0.0002, 'b1': 0.5, 'b2': 0.999, 'decay_epoch': 100, 'n_cpu': 8, 'hr_height': 256, 'hr_width': 256, 'channels': 3, 'sample_interval': 100, 'checkpoint_interval': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "read_epoch = 0\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"epoch\": read_epoch, # 시작 반복 위치 ( 만약 check point를 활용할 경우에 0이 아닌 다른 값을 사용하여 호출 하기 위함 )\n",
    "        \"n_epochs\": 200, # 전체 데이터셋 학습 횟수 (20만장의 이미지를 총 200번 학습)\n",
    "        \"dataset_name\": \"img_align_celeba\", # 학습 이미지 파일 위치\n",
    "        \"batch_size\": 8, # 한번에 8장의 이미지를 모델에 넣고 학습\n",
    "        \"lr\": 0.0002, # learning_rate 학습률\n",
    "        \"b1\": 0.5, # adam : decay of first order momentum of gradient\n",
    "        \"b2\": 0.999, # adam : decay of first order momentum of gradient\n",
    "        \"decay_epoch\":100, # epoch from which to start learning rate decay\n",
    "        \"n_cpu\": 8, # batch 생성에 사용할 cpu threads 수\n",
    "        \"hr_height\":256, # super resolution's resolution & image height\n",
    "        \"hr_width\":256, # super resolution's resolution & image width\n",
    "        \"channels\":3, # input image channel ( RGB ( COLOR Image ))\n",
    "        \"sample_interval\":100, # 100batch 마다 학습을 통해 update 된 모델을 활용하여 low resolution image를 high resolution으로 변환     \n",
    "        \"checkpoint_interval\" :1 # check point\n",
    "\n",
    "})\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda) # cuda 사용 가능일 경우 True 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_shape = (args.hr_height, args.hr_width) # (256, 256)\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorResNet()\n",
    "\n",
    "discriminator = Discriminator(input_shape=(args.channels, *hr_shape))\n",
    "#loss에 사용될 VGG19\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "criterion_GAN = torch.nn.BCEWithLogitsLoss() # model내에 sigmoid를 쓰지 않았기에 해당 BCE를 사용하여 sigmoid처리를 해줌\n",
    "criterion_content = torch.nn.MSELoss() # Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda: # cuda 사용 설정 (True일 경우 모든 model gpu사용할 수 있도록 설정하는 작업)\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_content = criterion_content.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.epoch != 0:  # 처음부터 학습이 아닐 경우에는 saved_models에서 해당 시작 위치에 해당하는 checkpoint 정보 가져오기\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers Adam\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=args.lr, betas=(args.b1, args.b2))  # generator optimizer\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=args.lr,betas=(args.b1, args.b2))  # discriminator optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor # cuda사용 유무에 따라서 input data형태 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(  # training data read\n",
    "    ImageDataset(\"../../data/%s\" % args.dataset_name, hr_shape=hr_shape,max_index=6000),# root = ../../data/img_align_celeba &  hr_shape = hr_shape\n",
    "    batch_size=args.batch_size,  # batch size ( mini-batch )\n",
    "    shuffle=True,  # shuffle\n",
    "    num_workers=args.n_cpu,  # using 8 cpu threads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOError\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epoch, args.n_epochs): # epoch ~ 200\n",
    "    try:\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "\n",
    "            # Configure model input\n",
    "            imgs_lr = Variable(imgs[\"lr\"].type(Tensor)) # low resolution\n",
    "            imgs_hr = Variable(imgs[\"hr\"].type(Tensor)) # high resolution\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "            #Generator 학습\n",
    "            optimizer_G.zero_grad()\n",
    "            # 모든 노드의 값들을 0으로 만들어준다\n",
    "            # Generate a high resolution image from low resolution input\n",
    "            gen_hr = generator(imgs_lr)\n",
    "            #low resolution input을 Gen에 입력으로 넣어 high resolution을 생성\n",
    "\n",
    "            # Adversarial loss\n",
    "            loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
    "\n",
    "            # Content loss\n",
    "            gen_features = feature_extractor(gen_hr) # Content loss에 사용될 data\n",
    "            real_features = feature_extractor(imgs_hr) # Content loss에 사용될 data\n",
    "            loss_content = criterion_content(gen_features, real_features.detach()) \n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_content + 1e-3 * loss_GAN\n",
    "\n",
    "            loss_G.backward() # backward\n",
    "            optimizer_G.step() # update\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            # Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Loss of real and fake images\n",
    "            loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
    "            loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            sys.stdout.write(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\\n\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
    "            )\n",
    "\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            if batches_done % opt.sample_interval == 0:\n",
    "                # Save image grid with upsampled inputs and SRGAN outputs\n",
    "                imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
    "                gen_hr = make_grid(gen_hr, nrow=1, normalize=False) # change normalize=True => False\n",
    "                imgs_lr = make_grid(imgs_lr, nrow=1, normalize=False) # normalize means that shift the image to the range(0,1), by the min and max values specified by range. Default = False\n",
    "                img_grid = torch.cat((imgs_lr, gen_hr), -1)\n",
    "                save_image(img_grid, \"images/%d.png\" % batches_done, normalize=False)\n",
    "\n",
    "        if epoch != 0 and epoch % opt.checkpoint_interval == 0 :\n",
    "            # Save model checkpoints\n",
    "            torch.save(generator.state_dict(), \"saved_models/generator_%d.pth\" % epoch)\n",
    "            torch.save(discriminator.state_dict(), \"saved_models/discriminator_%d.pth\" % epoch)\n",
    "    except IOError:\n",
    "        print(\"IOError\") # jupyter notebook에서 정상적으로 실행되지 않는 현상\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 관련 소스\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "un_mean = np.array([-(mean[0]/std[0]),-(mean[1]/std[1]),-(mean[2]/std[2])])\n",
    "un_std = np.array([1/std[0],1/std[1],1/std[2]])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, hr_shape, max_len=0): # download, read data 등등을 하는 파트\n",
    "        hr_height, hr_width = hr_shape # 256 X 256\n",
    "        if(max_len==0):\n",
    "            max_len = len(self.files)\n",
    "        self.max_len = max_len\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        self.lr_transform = transforms.Compose( # 고해상도로 만들 데이터인 low resolution data\n",
    "            [\n",
    "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC), # 기본 width, height에서 4를 나눈 값으로 resize\n",
    "                transforms.ToTensor(), # image값을 tensor형태로 변환\n",
    "                transforms.Normalize(mean, std), # 위에서 선언한 mean, std값을 사용하여 0~1사이로 Normalize\n",
    "            ]\n",
    "        )\n",
    "        self.hr_transform = transforms.Compose( # high resolution image 데이터\n",
    "            [\n",
    "                transforms.Resize((hr_height, hr_height), Image.BICUBIC), # BICUBIC : 주변 16개의 픽셀을 사용하여 처리\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(root + \"/*.*\")) # 오름차순으로 파일 정렬\n",
    "\n",
    "    def __getitem__(self, index): # 인덱스에 해당하는 아이템을 넘겨주는 파트\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img_lr = self.lr_transform(img)\n",
    "        img_hr = self.hr_transform(img)\n",
    "\n",
    "        return {\"lr\": img_lr, \"hr\": img_hr} # map 형태로 반환\n",
    "\n",
    "    def __len__(self): # data size를 넘겨주는 파트\n",
    "        #return len(self.files)\n",
    "        return self.max_len # 파일 길이 반환 ( 정한 이미지 수 )\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 178, 'n_epochs': 200, 'dataset_name': 'img_align_celeba', 'batch_size': 8, 'lr': 0.0002, 'b1': 0.5, 'b2': 0.999, 'decay_epoch': 100, 'n_cpu': 12, 'hr_height': 256, 'hr_width': 256, 'channels': 3, 'sample_interval': 100, 'checkpoint_interval': 1}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import easydict\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "read_epoch_non_sigmoid = 200\n",
    "read_epoch = 178\n",
    "opt = easydict.EasyDict({\n",
    "        \"epoch\": read_epoch, # 시작 반복 위치 ( 만약 check point를 활용할 경우에 0이 아닌 다른 값을 사용하여 호출 하기 위함 )\n",
    "        \"n_epochs\": 200, # 전체 데이터셋 학습 횟수 (20만장의 이미지를 총 200번 학습)\n",
    "        \"dataset_name\": \"img_align_celeba\", # 학습 이미지 파일 위치\n",
    "        \"batch_size\": 8, # 한번에 8장의 이미지를 모델에 넣고 학습\n",
    "        \"lr\": 0.0002, # learning_rate 학습률\n",
    "        \"b1\": 0.5, # adam : decay of first order momentum of gradient\n",
    "        \"b2\": 0.999, # adam : decay of first order momentum of gradient\n",
    "        \"decay_epoch\":100, # epoch from which to start learning rate decay\n",
    "        \"n_cpu\": 8, # batch 생성에 사용할 cpu threads 수\n",
    "        \"hr_height\":256, # super resolution's resolution & image height\n",
    "        \"hr_width\":256, # super resolution's resolution & image width\n",
    "        \"channels\":3, # input image channel ( RGB ( COLOR Image ))\n",
    "        \"sample_interval\":100, # 100batch 마다 학습을 통해 update 된 모델을 활용하여 low resolution image를 high resolution으로 변환     \n",
    "        \"checkpoint_interval\" :1 # check point\n",
    "\n",
    "})\n",
    "print(opt)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "hr_shape = (opt.hr_height, opt.hr_width)\n",
    "\n",
    "generator = GeneratorResNet()\n",
    "\n",
    "discriminator = Discriminator(input_shape=(opt.channels, *hr_shape))\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator_sigmoid = GeneratorResNet()\n",
    "\n",
    "discriminator_sigmoid = Discriminator_withDense(input_shape=(opt.channels, *hr_shape))\n",
    "feature_extractor_sigmoid = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor_sigmoid.eval()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "\n",
    "    generator_sigmoid = generator_sigmoid.cuda()\n",
    "    discriminator_sigmoid = discriminator_sigmoid.cuda()\n",
    "    feature_extractor_sigmoid = feature_extractor_sigmoid.cuda()\n",
    "\n",
    "\n",
    "if opt.epoch != 0:  # 처음부터 학습이 아닐 경우에는 saved_models에서 해당 시작 위치에 해당하는 checkpoint 정보 가져오기\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"%read_epoch_non_sigmoid))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"%read_epoch_non_sigmoid))\n",
    "    generator_sigmoid.load_state_dict(torch.load(\"saved_models_new/generator_%d.pth\"%opt.epoch))\n",
    "    discriminator_sigmoid.load_state_dict(torch.load(\"saved_models_new/discriminator_%d.pth\" % opt.epoch))\n",
    "\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "dataloader = DataLoader(  # training data read\n",
    "    ImageDataset(\"../../data/%s\" % opt.dataset_name, hr_shape=hr_shape,max_len=6000),# root = ../../data/img_align_celeba &  hr_shape = hr_shape\n",
    "    batch_size=opt.batch_size,  # batch size ( mini-batch )\n",
    "    shuffle=True,  # shuffle\n",
    "    num_workers=opt.n_cpu,  # using 8 cpu threads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, imgs in enumerate(dataloader):\n",
    "\n",
    "        # Configure model input\n",
    "        imgs_lr = Variable(imgs[\"lr\"].type(Tensor)) # low resolution\n",
    "        imgs_hr = Variable(imgs[\"hr\"].type(Tensor)) # high resolution\n",
    "\n",
    "        # Generate a high resolution image from low resolution input\n",
    "        gen_hr = generator(imgs_lr)\n",
    "        gen_hr_sigmoid = generator_sigmoid(imgs_lr)\n",
    "\n",
    "        unorm = UnNormalize()\n",
    "        imgs_lr = unorm(imgs_lr)\n",
    "        imgs_hr = unorm(imgs_hr)\n",
    "        gen_hr = unorm(gen_hr)\n",
    "        gen_hr_sigmoid = unorm(gen_hr_sigmoid)\n",
    "\n",
    "        imgs_lr_neareset = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
    "        #imgs_lr_linear = nn.functional.interpolate(imgs_lr, scale_factor=4, mode='linear',align_corners=False)\n",
    "        imgs_lr_bilinear = nn.functional.interpolate(imgs_lr, scale_factor=4, mode='bilinear',align_corners=False)\n",
    "        #imgs_lr_trilinear = nn.functional.interpolate(imgs_lr, scale_factor=4, mode='trilinear',align_corners=False)\n",
    "        imgs_lr_bicubic = nn.functional.interpolate(imgs_lr,scale_factor=4,mode='bicubic',align_corners=False)\n",
    "\n",
    "        gen_sr = make_grid(gen_hr, nrow=1, normalize=False) # change normalize=True => False\n",
    "        imgs_hr = make_grid(imgs_hr, nrow=1, normalize=False)\n",
    "        gen_hr_sigmoid = make_grid(gen_hr_sigmoid, nrow=1, normalize=False)\n",
    "        #imgs_lr_linear = make_grid(imgs_lr_neareset, nrow=1, normalize=False)\n",
    "        imgs_lr_bilinear = make_grid(imgs_lr_neareset, nrow=1, normalize=False)\n",
    "        #imgs_lr_trilinear = make_grid(imgs_lr_neareset, nrow=1, normalize=False)\n",
    "        imgs_lr_neareset = make_grid(imgs_lr_neareset, nrow=1, normalize=False)\n",
    "        imgs_lr_bicubic = make_grid(imgs_lr_neareset, nrow=1, normalize=False)\n",
    "\n",
    "        #imgs_lr = make_grid(imgs_lr, nrow=1, normalize=False) # normalize means that shift the image to the range(0,1), by the min and max values specified by range. Default = False\n",
    "        img_grid = torch.cat((imgs_lr_bilinear,imgs_lr_neareset,imgs_lr_bicubic, gen_sr,gen_hr_sigmoid, imgs_hr), -1)\n",
    "        # bilinear / nearest / bicubic / gen_sr(L1Loss(Feature Extract),MSE) / gen_hr_sigmoid (MSE(Feature Extract), BCE ) / original \n",
    "        save_image(img_grid, \"test_images/%d.png\" % i, normalize=False)\n",
    "        if(i==10):\n",
    "            exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교분석\n",
    "#### Bilinear / Nearest / Bicubic / SR ( L1Loss(FE) & MSE ) / SR ( MSE(FE) & BCE ) / SR ( MSE(FE) & BCE ) / Original \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./test_images/0.png\"\\>\n",
    "    <img src=\"./test_images/1.png\"\\>\n",
    "    <img src=\"./test_images/2.png\"\\>\n",
    "    <img src=\"./test_images/3.png\"\\>\n",
    "    <img src=\"./test_images/4.png\"\\>\n",
    "    <img src=\"./test_images/5.png\"\\>\n",
    "    <img src=\"./test_images/6.png\"\\>\n",
    "    <img src=\"./test_images/7.png\"\\>\n",
    "    <img src=\"./test_images/8.png\"\\>\n",
    "    <img src=\"./test_images/9.png\"\\>\n",
    "    <img src=\"./test_images/10.png\"\\>\n",
    "    <img src=\"./test_images/11.png\"\\>\n",
    "    <img src=\"./test_images/12.png\"\\>\n",
    "    <img src=\"./test_images/13.png\"\\>\n",
    "    <img src=\"./test_images/14.png\"\\>\n",
    "    <img src=\"./test_images/15.png\"\\>\n",
    "    <img src=\"./test_images/16.png\"\\>\n",
    "    <img src=\"./test_images/17.png\"\\>\n",
    "    <img src=\"./test_images/18.png\"\\>\n",
    "    <img src=\"./test_images/19.png\"\\>\n",
    "    <img src=\"./test_images/20.png\"\\>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위의 예시의 각 GAN들의 학습 횟수가 동일하지 않기 때문에 정확한 비교는 아닙니다.\n",
    "### - interpolation방식들 보다는 보다 자연스러운 이미지로 SR을 함 (4,5,6)\n",
    "### - model의 구성은 거의 유사하지만 loss function을 어떻게 쓰냐에 따라서 크게 차이를 보임.\n",
    "### - 같은 epoch학습 크기로 비교하지 못해서 ( 학습 시간적 여유 ) 정확한 비교는 아니지만 L1_loss를 contentloss로 사용하였을 때 가장 좋은 성능을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 결론\n",
    "\n",
    "### - SRGAN을 통해서 저해상도 이미지를 고해상도로 복구한 경우 기존에 존재한 interpolation들을 사용한 방법보다는 보다 자연스러운 이미지로 복구가 가능함\n",
    "### - 하지만 학습 과정에서의 문제 혹은 논문과는 완벽하게 일치하지 않는 학습 방법으로 학습 또는 같은 데이터셋으로 훈련을 했는지에 대한 부분에 따라서 다른 결과를 보임 ( 색감 쪽에서 좋지 않은 성능을 보임 )\n",
    "### - 결과적으로 자연스러운 이미지를 복구 할 수 있음을 육안으로 확인할 수 있었기에 좋은 결과라고 판단\n",
    "### - 추후 다른 SR 관련 GAN을 추가적으로 공부할 예정\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "### - https://pytorch.org/ : pytorch \n",
    "### - https://developer.nvidia.com/cuda-10.1-download-archive-base : cuda tool kit\n",
    "### - https://developer.nvidia.com/cudnn : cudnn\n",
    "### - https://github.com/eriklindernoren/PyTorch-GAN : GAN관련 open source\n",
    "### - https://github.com/dongyyyyy/PyTorch-GAN/tree/master/implementations/srgan : open source를 참조하여 수정한 내용\n",
    "### - https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356 : VGG19 모델 이미지\n",
    "### - Photo-Realistic Single Image Super-Resolution Using a generative Adversarial Network. Christian Ledig, Lucas Theis et al.; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 4681-4690\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.3.1-cuda10.1",
   "language": "python",
   "name": "pytorch-cuda10.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
